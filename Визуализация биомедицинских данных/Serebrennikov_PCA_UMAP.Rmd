---
title: 'Эксплораторный анализ - 1'
author: "Дмитрий Серебренников"
output: 
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Загрузим библиотеки
library(dplyr)
library(ggplot2)
theme_set(theme_minimal())
library(ggpubr)
```

# Загрузим данные и пакеты

```{r}

pima <- openxlsx::read.xlsx('data/pima.xlsx')

summary(pima)
```


```{r}
# Сделаем более детализированную переменную возрастных групп
pima <- pima %>% 
  mutate(
    age_group = case_when(
      age < 31 ~ "21-30",
      age >= 31 & age < 41 ~ "31-40",
      age >= 41 & age < 51 ~ "41-50",
      age >= 51 & age < 61 ~ "51-60",
      age >= 61 ~ "60+"
    ))
table(pima$age_group)
```

...

# Анализ корреляций

Когда мы говорим об отношениях переменных друг к другу, мы так или иначе приходим к языку корреляции. Т.е. статистической взаимосвязи двух или более случайных величин при которых изменения в одной переменной связаны с изменением во второй.

На всякий случай привожу формулу рассчёта коэффициента корреляции Пирсона:

$r_{xy}=\frac{\Sigma(x_i-\bar{x})\times(y_i-\bar{y})}{\sqrt{\Sigma(x_i-\bar{x})^2\times\Sigma(y_i-\bar{y})^2}}$

Он может принимать значения от -1 (отрицательная связь), до 1 (положительная связь).

В R можно строить матрицу корреляций для всех численных переменных нашего датасета с помощью пакета `corrplot`. Загрузим его:

```{r}
library(corrplot)
```

Сама матрица строится в 2 этапа.

1.  Получаем объект матрицы:

```{r}
# Для более "чистого" результата, избавляемся от ошибочных значений
pima_clear <- pima %>% 
  filter(glucose != 0 & pressure != 0 & triceps != 0 & insulin != 0 & mass != 0 & age != 0 ) %>% 
  select(is.integer | is.numeric) # Обратите внимание, в dplyr можно задавать выборку колонок через команды определения формата данных

# Сделаем ещё один "чистый" датафрейм, но со значениями диабет статуса. Он понадобится нам в самом конце занятия 
pima_clear_with_ch <- pima %>% 
  filter(glucose != 0 & pressure != 0 & triceps != 0 & insulin != 0 & mass != 0 & age != 0 )


head(pima_clear)
```

```{r}
# Получаем непосредственно матрицу
pima_cor <- cor(pima_clear) 
pima_cor
```

2.  Визуализируем её в corplot:

```{r}
corrplot(pima_cor, method = 'number')
```

У представления такой матрицы может быть ещё большое количество вариаций. Их можно посомтреть [здесь](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html).

Например:

```{r}
corrplot(pima_cor, method = "color", type = "lower", 
         addCoef.col = "grey30", diag = FALSE,
         cl.pos = "b", tl.col = "grey10",
         col = COL2('RdBu', 10))
```

Здесь мы видим 3 пары сильно скоррелированных между собой переменных.


Кроме того, я рекомендую посмотреть также на пакет [corrr](https://drsimonj.svbtle.com/exploring-correlations-in-r-with-corrr).

В нём реализованы более дателизированные функции для анализа корреляций. Например функция ниже показывает не только корреляционные взаимосвязи, но и отношения близости переменных друг к другу с точки зрения сетевого анализа:

```{r}
library(corrr)

pima_cor %>% 
  network_plot(min_cor = .0)
```

Такие графики осмысленно использовать тогда когда у вас небольшое (до 20) количество переменных.

...

## Матричные графики

С помощью таких графиков можно получить информацию как о распределении отдельных количественных переменных, так и о взаимосвязи внутри каждой пары количественных переменных.

Можно использовать функцию `ggpairs` из пакета `GGally`:

```{r}
library(GGally)
ggpairs(pima_clear, progress = F)
```


# Heat map

## Анализ наблюдений

Второй способ использовать heat map - представлять сами наблюдения по какому-то универсальному признаку.

Например, мы можем применить процедуру стандартизации значений для всех наблюдений по численным переменным и визуализировать его.

Стандартизируем значения:

```{r}
pima_clear_scaled <- scale(pima_clear)
head(pima_clear_scaled)
```

Визуализируем с помощью расширения к ggplot - ggfortify(). В чем
проблема такого графика?

```{r}
library(ggfortify) 

autoplot(pima_clear_scaled)
```

...

# Поиск схожих наблюдений. Лирическое отступление об основах кластерного анализа

# Heat map + Tree map?

Мы затронули основы кластеризации, чтобы понять, как работать в тех случаях, когда нам нужно упорядочить данные, сгруппировав наблюдения. Совместив это с heat map мы можем выделить группы наблюдений по каким-либо признакам.

Сделать это просто через пакет pheatmap.

Загрузим его.

```{r}
library(pheatmap)
```

Визуализируем heat map и tree map одновременно:

```{r}
# считаем расстояния между точками
pima_clear_dist <- dist(pima_clear_scaled)

# строим график
pheatmap(pima_clear_scaled, 
         show_rownames = FALSE, 
         clustering_distance_rows = pima_clear_dist, # матрица дистанций
         clustering_method = "ward.D2", 
         cutree_rows = 5, # количество групп(кластеров)
         cutree_cols = length(colnames(pima_clear_scaled)),
         angle_col = 45, 
         main = "Dendrograms for clustering rows and columns with heatmap")
```





# Principal component analysis (PCA)

Сейчас мы уже можем выделить определённые группы пациентов. Однако даже на таком маленьком объёме данных (напомню, мы работаем с 392 наблюдениями) график почти не читаем. Что делать?

Одно из самых популярных решений - метод главных компонент (principal component analysis или PCA).

...

## Теория и учебный пример

Чтобы разбраться с методом рассмотрим какие две проблемы он решает и как он это делает:

1.  Если в данных есть ряд скоррелированных переменных (например, в нашем случае - mass и triceps), то они не дают провести корректные оценки данных и выделение паттернов.


Почему?

2.  Если в данных очень много наблюдений и переменных, то при разведывательном анализе необходимо сократить их размер, не потеряв в качестве и полноте интерпретаций. Эту задачу выполняют методы уменьшения размерности и PCA относится к ним.

...

Рассмотрим учебный пример. Допустим у нас в данных есть только колонки mass и triceps. Мы хотим с одной стороны снять корреляцию между ними, а с другой - уменьшить размерность, т.е. представить их одной переменной.

    Как нам это сделать?

...

**Мы можем сконструировать новые переменные из двух имеющихся и сняв корреляцию и уменьшив размерность!**

Сделать так, чтобы линия тренда стала горизонтальной вокруг нуля -  как будто нулевая корреляции 

```{r}
# Подготовим данные
pima_example <- pima_clear %>% 
  select(mass, triceps)

# Визуализируем
ggplot() +
  geom_point(data = pima_example, aes(x = mass, y = triceps)) +
  theme_minimal()

# Что значит такое распределение точек? 
```

**Сконструируем переменные, в которых не будет корреляции и которые (делаем из бага фичу!) помогут уменьшить размерность**

Если мы хотим привести две переменные к одной, то мы должны провести какое-то действие с этими переменными. Например, сложить!

```{r}
pima_example <- pima_example %>% 
  mutate(pc1 = mass + triceps) # Создаем первую главную компоненту
```

А давайте теперь поэксперементируем с вычитанием

```{r}
pima_example <- pima_example %>% 
  mutate(pc2 = mass - triceps)
```

Теперь ещё раз вспомним как выглядит график:

```{r}
ggplot() +
  geom_point(data = pima_example, aes(x = mass, y = triceps)) + 
  theme_minimal() 
```

    А теперь, следим за руками...

    А что если мы просто сделаем колонки, которые только что создали (pc1, pc2) нашими новыми координатными осями?
    Т.е. вместо x будет pc1, вместо y - pc2, а точки перерисуются исходя из нашего нового представления. 

**Вспоминаем две проблемы, с которых мы начинали. Как такое преобразование помогает решить обе сложности?**

...

Чтобы не быть голословными, перерисуем график, но уже с новыми координатами - pc1 и pc2:

```{r}
ggplot() +
  geom_point(data = pima_example, aes(x = pc1, y = pc2)) +
  theme_minimal() 

# Догадайтесь, где теперь лежат средние линии наших старых координат (mass, triceps)?
# Кажется корреляция осталась (отрицательная), а что сделать с переменными, что занулить её?
```

Вопросы на понимание:

-   А зачем мы это сделали? Какие следствия такого подхода?

-   Если мы хотим выразить две наши переменные какой-то одной, то что мы выберем pc1 или pc2?

...

### Подойдём к вопросу более формально

-   Нам нужно найти оси, которые приведут корреляцию к нулю и при этом, создадут макисмальную полноту разброса переменных. Т.е. фактически, в нашей задаче мы решем два уравнения:

$PC1 = (alpha_1 * mass) + (beta_1 * triceps)$

$PC2 = (alpha_2 * mass) + (beta_2 * triceps)$

...где alpha и beta - это коэффициенты, которые позволяют добиться
минимальной скоррелированности mass и triceps. (читай - найти такой угол для новых осей координат, где в данных будет максимальный разброс при минимальной корреляции)

Воспользуемся специальной функцией для этого:

```{r}
pima_example <- pima_example %>% 
  select(mass, triceps) # Оставим только нужные нам две переменные


# prcomp - это функция, которая находит главные компоненты
pima.pca <- prcomp(pima_example, 
                scale = T) # шкалировать данные, чтобы все данные были как будто в одной шкале

#Нужно ли нормировать? TRUE|FALSE. Фактически, мы могли бы подать на функцию pima_example_scaled и тогда следовало поставить scale = F, веть нормирование в том датафрейме уже сделано!
```

    NB: Помните, что нормирование перед работой с PCA делает этот метод затруднительным для работы с бинарными переменными (почему?). У этого есть ряд выходов. Для подсчёта матрицы дистанций можно использовать gower distance. А вместо PCA использовать FAMD (Factorial Analysis of Mixed Data), основанный на комбинации PCA для количественных переменных и MCA (Multiple Correspondence Analysis) для категориальных.
    
    
PCA показывает у каких переременных, где в данных наибольая дисперсия. Т.е. где наибольшая вариативность. 
Мы как бы делаем 3Д визуализацию для тех переменных, которые мы хотим преобразовать в компоненту, и проводим много много раз линии через начало координат и ищут ту линию, которая даст наибольшую дисперсию.

Линия - это новая переменная - наша компонента. Эта линия дала эти самые коэффициенты альфа и бета, которые мы искали. Мы ищем такую новую переменную, которая максимальным образом вберет в себя вариацию исходных переменных.

То есть в нашем случае мы хотим найти новую переменную, которая максимально вберет в себя вариацию массы и трицепса, но при этом не будет коррелировать с ними. (мы можем брать 3, 4 и тд сколько хотим переменных и искать для них компоненту)

Компонента сама по себе не имеют смысла, но она позволяет нам увидеть, какие переменные вносят наибольший вклад в наши данные.

Это главная компонента - с наивысшей корреляцией. 
Вторая главная компонента - с наивысшей корреляцией, но при этом не коррелирующая с первой. 
Третья - с наивысшей корреляцией, но при этом не коррелирующая с первой и второй и тд.

Компоненты будут перпендикулярны друг другу в 3Д пространстве.

Максимальное количество компонент = количество исходных переменных (количественных), потому что чтобы описать максимально хорошо вариацию в 3 переременных нам понадобятся 3 переменных.

Но сколько по факту брать компонент для анализа - решаем сами.

*Как же определить коэффициенты альфа и бета, чтобы домножить их на наши переменные и убрать корреляцию?*

Посмотрим на веса наших главных компонент:

```{r}
pima.pca$rotation
```

Rotation показывает коэффициенты разворота новых осей относительно
старых. Таким образом, решением нашего уравнения будет:

$PC1 = (0.707 * mass) + (-0.707 * triceps)$

$PC2 = (0.707 * mass) + (0.707 * triceps)$

*(странность весов обусловлена тем, что мы работаем с учебным примером, где всего лишь 2 переменные)*

Нарисуем компоненты на графике:

```{r}
ggplot() +
  geom_point(data = pima.pca$x, # Новые переменные лежат здесь
             aes(x = PC1, y = PC2)) +
  theme_minimal() 
```

Но как функция высчитала эти цифры?

1.  Она находит центр распределения данных по среднему двух переменных;

2.  От него строит множество прямых линий с каждым разом делая угол наклона всё больше;

3.  Для каждой линии считается сумма квадратов расстояний от всех точек в данных до линии;

4.  Выбирается линия, где эта сумма будет минимальной. Она и становится новой главной компонентой;

5.  Если алгоритм уже отобрал однй главную компоненту, он начинает отбирать следующую так, чтобы каждая следующая линия имела наименьшую корреляцию со всеми предыдущими;

6.  Алгоритм останавливается, тогда, когда количесто найденных главных компонент не сравняется с количеством переменных.

...

    Но в чём суть, если мы хотели уменьшить количество переменных, а оно (количество главных компонент) осталось таким же?

    - Да, осталось, но только вот главные компоненты концептуально неравны друг другу. Каждая следующая из них объясняет меньше предыдущей. Считается, что алгоритм сработал хорошо, когда 70% вариации данных укладывается в 3 компоненты. 

    ...Т.е. компонент может быть десятки, но первые 3 агреггируют в себе большую часть сложности наших данных. 

Это легко заметить уже даже в случае трёх переменных (и трёх компонент).

Рассмотрим прекрасный материал Гарика Мороза и соавторов по ссылке [здесь](http://math-info.hse.ru/f/2015-16/ling-mag-quant/lecture-pca.html), кроме того я совутую [эту интерактивную визуализацию PCA](https://bryanhanson.github.io/LearnPCA/articles/Vig_05_Visualizing_PCA_3D.html) или [эту](https://setosa.io/ev/principal-component-analysis/).

    ```{r}
    # Кстати, мы также можем делать 3d визуализацию с помощью plotly:
    plot_ly(data = pima_clear, 
            x=~mass, 
            y=~pressure, 
            z=~triceps, 
            size = 1,
            type="scatter3d", mode="markers")
    # Но это совсем другая тема...
    ```

...

## Практика и реальный пример

Проведём PCA уже на полных данных pima

```{r}
# Загрузим библиотеки
library(FactoMineR)
```

Делаем PCA:

```{r}
pima_full.pca <- prcomp(pima_clear, 
                        scale = T) # Не забываем про стандартизацию!
```

Оценим результат.

```{r}
summary(pima_full.pca)
```

Смотрим на "Cumulative Proportion" - сколько в совокупности компоненты описывают вариации. 

Правило:

Считается, что если первые две компоненты объясняют 60-70% вариации, то это хороший результат. Если 90% - отлично. 

Если первые три компоненты объясняют 70% вариации, то уже не очень хорошо. Если меньше - метод PCA не подходит для анализа наших данных.

100% всегда достигается в последней колонке (их количество равно количеству переменных в данных) и последняя компонента будет объяснять очень мало вариации (2% например).

Если кумулятивная пропорция вариации у первых двух компонент равна 0.7, это значит что 70% вариации данных объясняются этими двумя компонентами. Это также означает, что переменные в данных сильно скоррелированы между собой и PCA как раз снимает эту корреляцию, вбирает в себя вариацию.

У нас в данных первые 4 главные компоненты объясняют 74% вариации данных. Посмотрим это на графике:

```{r}
library(factoextra)

fviz_eig(pima_full.pca, addlabels = T, ylim = c(0, 40))
```

На самом деле, это не слишком хороший результат, т.к. следующая конвенционально важная отметка в 90% достигается уже только на PC7. Бывают данные, которые не слишком хороши для PCA, но нельзя сказать, что у нас всё ужасно. Первые две компоненты объясняют 50% дисперсии. Вокруг них и будет сосредоточен основной анализ.

Плюсы PCA:

1. Мы можем понять отношение переменных между друг другом.
2. Мы можем понять отношение строк между собой исхожя из новых компонент.
3. И строки, и колонки аппелируют к компонентам, что позволяет нам понять, поэтому PCA единственный метод, который может анализировать колонки и строки одновременно.

### Анализ переменных по PCA

Мы можем начать анализировать, как наши переменные связаны с PC1 и PC2.
Посмотрим на график ниже:

```{r}
fviz_pca_var(pima_full.pca, col.var = "contrib")
```
Стрелки показывают, какие переменные (вектора) имели наиболую вариацию изначально и соответственно вносят наибольший вклад в PC1 и PC2.

Мы видим, что вектора сгруппированы в 3 группы. 

Если мы видим, что вектора направлены в разные стороны (180 градусов), то это значит, что переменные отрицательно скоррелированы между собой.

Если вектора образуют между собой прямой угол, то это значит, что переменные не связаны между собой.

Если вектора направлены в одну сторону, то это значит, что переменные положительно скоррелированы между собой.

Чем длиннее стрелка, тем больше вклад переменной в компоненты.

Подсказка для интерпретации графика
[здесь](https://bioinfo4all.files.wordpress.com/2021/01/principal-component-analysis-pca-1.png?w=2048).

-   Стрелки - средние значения переменных для PC1 (Dim1) и PC2 (Dim2). В скобках указаны проценты объяснённой дисперсии каждой из двух компонент. На каждую последующую PC всегда приходится всё меньше и меньше разброса в данных.
-   Цвет и близость к кругу - насколько та или иная переменная вносит вклад в анализируемые главные компоненты
-   Направление - относительная мера близости переменных. Если стрелки расходятся в прямо-противоположные стороны, то переменные отрицательно скоррелированы внутри представленных главных компонент.

В данных мы видим три группы переменных:

-   age, pregnant.

-   mass, triceps, pedigree

-   остальные

    Теперь оценим PC1 и PC2 в целом. Что "схватили" первая и вторая
    компоненты? какой разброс мы видим по осям x и y?

...

Мы также можем отдельно посмотреть на, например, топ 3 самых важных
переменных с т.зр. их вариации в PC1 и PC2:

```{r}
fviz_pca_var(pima_full.pca, 
             select.var = list(contrib = 3), # Задаём число переменных здесь 
             col.var = "contrib")
```

    У самих по себе главных компонент есть одна очень большая проблема с точки зрения их анализа. Какая?

Посмотрим из чего состоят 1, 2 и 3 главные компоненты:

```{r}
fviz_contrib(pima_full.pca, choice = "var", axes = 1, top = 24) # 1
fviz_contrib(pima_full.pca, choice = "var", axes = 2, top = 24) # 2
fviz_contrib(pima_full.pca, choice = "var", axes = 3, top = 24) # 3
```

### Анализ наблюдений по PCA - biplot

Помимо переменных мы можем анализировать также и наблюдения, искать в них кластеры и корреляцию с переменными в целом. Для этого используется biplot. Bi потому, что на нем одновременно изображены и точки, и переменные

```{r}
# Загрузим библиотеку
library(ggbiplot) 
```

Сделаем biplot:

```{r}
ggbiplot(pima_full.pca, 
         scale=0, alpha = 0.1) + 
  theme_minimal()
```
Точки - это наблюдения, стрелки - переменные. 

Стрелки выходят из центра графика (0) и указывают в направлении наибольшего разброса переменных.

В этом графике длина стрелок неважна, это как бесконечный вектор, важно только направление.


Более осмысленным biplot становится при использовании кластерных
методов, с помощью которых мы можем разделить наблюдения на группы.

Наложим облака по типу группы (diabetes). diabetes переменная не была включена в PCA, но мы можем посмотреть, как она влияет на группировку наблюдений.


Посмотрим, наблюдается ли разница между группами по diabetes:

```{r}
# Сделаем корректные данные для группировки по diabetes.
pima_clear_with_ch <- pima %>% 
  filter(glucose != 0 & pressure != 0 & triceps != 0 & insulin != 0 & mass != 0 & age != 0 )

# Визуализируем с группировкой по diabetes (для этого переменную нужно сделать фактором)
ggbiplot(pima_full.pca, 
         scale=0, 
         groups = as.factor(pima_clear_with_ch$diabetes), 
         ellipse = T,
         alpha = 0.2) +
  theme_minimal()
```

Что мы видим?

Группа диабет+ скорее всего больше всего связана с бОльшим возрастом и числом беременностей,
менее связана с массой телой, педигри (родословная) и трицепсом,
a инсулин, глюкозa и давление требует прояснения.

???

А что с возрастными группами:

```{r}
ggbiplot(pima_full.pca, 
         scale=0, 
         groups = as.factor(pima_clear_with_ch$age_group), 
         ellipse = T,
         alpha = 0.2) +
  theme_minimal()
```
Что видим?

Вектор возраста идет в том же направлении, что и порядок возрастных групп. 




Для дальнейшего ознакомления с PCA я рекомендую посмотреть следующие туториалы:

[1](https://bioinfo4all.wordpress.com/2021/01/31/tutorial-6-how-to-do-principal-component-analysis-pca-in-r/),
[2](https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff),
[3](https://juliasilge.com/blog/best-hip-hop/),
[4](https://juliasilge.com/blog/cocktail-recipes-umap/)


...


# UMAP

PCA - отличный метод, когда мы хотим одновременно понять соотношения колонок и строк, но часто бывают ситуации, когда вам лучше понять близость строк друг к другу и сделать "сгустки" наблюдений, а не разряженные облака. Для таких задач применяют UMAP.

UMAP (Uniform Manifold Approximation and Projection) - это алгоритм уменьшения размерности, основанный на методах теории топологии. В отличие от PCA, он оценивает не глобальное отношение переменных, а локальную близость строк. Сначала мы оцениваем многомерное пространство, а затем по-очереди начинаем уменьшать его размерность, но так, чтобы при каждом следующем снижении изначально близкие друг другу точки становились ещё ближе (образуем своеобразные воронки).

Важно (!), UMAP имеет тенденцию сохранять локальные расстояния между точками в ущерб глобальному отображению (т.е. имеет тенденцию создавать "сгустки", а не разреженные облака). Благодаря этому он отлично подходит для понимания структуры отношения строк, но при этом он даёт крайне ограниченное количество способов анализировать колонки.

Вы можете прочитать детальнее [здесь](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html).

### Tidymodels approach

```{r, message=FALSE,warning=FALSE}
library(tidymodels)
library(embed)

umap_prep <- recipe(~., data = pima_clear) %>% # "техническая" строка, нужная для работы фреймворка tidymodels
  step_normalize(all_predictors()) %>% # нормируем все колонки
  step_umap(all_predictors()) %>%  # проводим в UMAP. Используем стандартные настройки. Чтобы менять ключевой параметр (neighbors), нужно больше погружаться в машинное обучение
  prep() %>%  # "техническая" строка, нужная для работы фреймворка tidymodels. Мы выполняем все степы выше 
  juice() # Финальная строка - приводим результаты UMAP к стандартизированному датасету

```

Визуализиуем два первых измерения UMAP и добавим информацию о возрастных группах и диабет-статусе:

```{r}
umap_prep %>%
  ggplot(aes(UMAP1, UMAP2)) + #  # можно добавить раскраску 
  geom_point(aes(color = as.character(pima_clear_with_ch$age_group),
                 shape = pima_clear_with_ch$diabetes_ch), 
             alpha = 0.7, size = 2) +
  labs(color = NULL) 
```





...

# К вопросу о продвинутой визуализации

**А если графики используются не для эксплораторного анализа, а для
презентации результатов?**

В визуализации данных есть своя теория и свои исследования воприятия разных графиков (например что-то вы можете почитать на [data-to-viz в блоге](https://www.data-to-viz.com/caveats.html) или у [Анастасии Кузнецовой](https://nastengraph.medium.com/)).

Когда график для вас - способ представить результат вашим коллегам или широкой публике, его стоит делать исходя из несколько других
соображений, чем когда вы делаете это при эксплораторном анализе. По этой причине, при подготовке графика следует учесть следующие принципы:

1.  Оцените насколько хорошо ваши данные подходят типу графика.

2.  Делайте фокус на чем-то одном: общих паттернах или деталях. Исходя из этого стоит выбирать тип графика.

3.  Агрегируйте большие объемы данных при визуализации.

4.  Правильно выбирайте палетки. Учитывайте то, как их могут читать дальтоники или, например, акцентируют ли цвета внимание читателя на том, что вам нужно?

5.  Не делайте график впечатлительным без необходимости. Эффектность в простоте.

6.  Убедитесь, что график соответствует интуитивным конвенциям восприятия (у него не перевёрнуты оси, они не обрезаны (но иногда это позволительно), у данных указан источник, все подписи унифицированны и проч.)

7.  При интерпретации результатов помните - график показывает только то, что он показывает. Ни один график не показывает вам причинных эффектов. Только связи. Correlation != Causation.

8.  Помните, что цель любого хорошего графика -- рассказать историю (и убедить вас в ней).

...

Я попытался рассказать историю на наших данных на графике ниже.
Попробуйте и вы!

```{r}
plot <- pima %>% 
  mutate(
    age_group = factor(age_group, levels = c("21-30", "31-40", "41-50", "51-60", "60+" )),
    diabetes = case_when(
      diabetes == 'pos' ~ "Diabet-Positive",
      diabetes == 'neg' ~ "Diabet-Negative"
    ) # Переводим в фактор, для корректной последовательности категорий
    ) %>% 
  filter(insulin != 0 & glucose != 0) %>% 
  ggplot(aes(x=insulin, y=glucose, color = age_group)) + 
  geom_point(size = 3, alpha = 0.8) + 
  facet_grid(. ~ diabetes) +
  scale_color_brewer(palette = 'OrRd') +
  ggtitle('Indian Women Diabets') +
  labs(y = 'Plasma glucose concentration (log10)', x = '2-Hours serum insulin (mu U\\ml) (log10)') + 
  guides(color = guide_legend(title = 'Age Groups')) +
  scale_x_log10() + scale_y_log10() +
  theme_minimal() 

plot
```

# Полезные ссылки по теме визуализации данных

-   [R Graph Galery](https://r-graph-gallery.com/index.html)

-   Ссылки на Телеграмм каналы:

    -   <https://t.me/revealthedata>
    -   <https://t.me/nastengraph>
    -   <https://t.me/data_publication>
    -   <https://t.me/leftjoin>
