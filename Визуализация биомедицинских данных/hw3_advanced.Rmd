---
title: "HW Advanced dataviz"
author: "Victoria Zaitceva"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggpubr)
library(factoextra)
theme_set(theme_minimal())
```


```{r}

data <- readRDS("C:/Users/VictoriaZaitceva/Desktop/personal/BioStat_2024/Визуализация биомедицинских данных/data/very_low_birthweight.RDS")
data <- readRDS("data/very_low_birthweight.RDS")

getwd()
```

```{r}
summary(data)
```

Сделайте копию датасета, в которой удалите колонки с количеством пропусков больше 100, а затем удалите все строки с пропусками. 


```{r}

data_clean <- data |>
  select(-where(~sum(is.na(.x)) > 100)) |>
  drop_na()


data_clean <- data_clean |>
  mutate(across(
    where(~ is.numeric(.x) && max(.x) == 1 && min(.x) == 0),
         ~ as.factor(.)
  ))

summary(data_clean)
```
Постройте графики плотности распределения для числовых переменных. Удалите выбросы, если таковые имеются. Преобразуйте категориальные переменные в факторы. Для любых двух числовых переменных раскрасьте график по переменной ‘inout’.


```{r}

vars <- data_clean |> select(where(is.numeric)) |> names()


plot_list <- lapply(vars, function(var) {
  ggplot(data_clean, aes_string(x = var, fill = "inout")) +
    geom_density(alpha = 0.5)
})


ggarrange(plotlist = plot_list, ncol = 3, nrow = 5)
```


```{r}

ggplot(data_clean, aes(x = lowph, fill = inout)) +
  geom_histogram()


data_clean <- data_clean |>
  filter(lowph > mean(lowph, na.rm = TRUE) - 3 * sd(lowph, na.rm = TRUE) & 
           lowph < mean(lowph, na.rm = TRUE) + 3 * sd(lowph, na.rm = TRUE))


```


### Проведите тест на сравнение значений колонки ‘lowph’ между группами в переменной inout. Вид статистического теста определите самостоятельно. Визуализируйте результат через библиотеку 'rstatix'. Как бы вы интерпретировали результат, если бы знали, что более низкое значение lowph ассоциировано с более низкой выживаемостью?


```{r}
# t test lowph by inout

t.test(lowph ~ inout, data = data_clean)

#Визуализируйте результат через библиотеку 'rstatix'
library(rstatix)

t <- data_clean |> 
  t_test(lowph ~ inout)


p <- ggplot(data_clean, aes(x = inout, y = lowph)) +
  geom_boxplot(aes(fill = inout), alpha = 0.7) +
  scale_fill_manual(values = c("red", "blue"))

p + stat_pvalue_manual(
  t,
  label = "p = {p}",
  y.position = max(data_clean$lowph, na.rm = TRUE) * (1.1))
```


### 4. Сделайте новый датафрейм, в котором оставьте только континуальные или ранговые данные, кроме 'birth', 'year' и 'exit'. Сделайте корреляционный анализ этих данных. Постройте два любых типа графиков для визуализации корреляций.


```{r}

for (name in colnames(data_clean)) {
  print(paste(name, ":", class(data_clean[[name]])))
}


final_data <- data_clean |>
  select(where(~ is.numeric(.x) | is.integer(.x))) |>
  select(-birth, -year, -exit)
```


```{r}
library(GGally)

ggpairs(final_data)
```

```{r}
library(corrplot)

final_data %>% 
  cor() %>% 
  corrplot(
    order = 'hclust'
  )

```


```{r}
library(corrr)

final_data %>% 
  cor() %>% 
  network_plot(min_cor = .0)

```
Наблюдается умеренно сильная корреляция между bwt и gest.


### 5. Постройте иерархическую кластеризацию на этом датафрейме.

```{r}
# 1. Scale


final_data_scaled <- final_data |> 
  scale()

# 2. Distance matrix

dist_matrix <- dist(final_data_scaled, 
                    method = "euclidean")


# 3. Высчитываем дендограмму кластеров 

hc <- hclust(dist_matrix, 
             method = "ward.D2")

# 4. Визуализируем дендограмму

fviz_dend(hc,
          k = 3, 
          cex = 0.4, 
          horiz = TRUE, 
          k_colors = "jco",
          rect = TRUE, 
          rect_border = "jco", 
          rect_fill = TRUE)

```



### 6. Сделайте одновременный график heatmap и иерархической кластеризации. Интерпретируйте результат.

```{r}
library(pheatmap)

pheatmap(final_data_scaled, 
         show_rownames = FALSE, 
         clustering_distance_rows = dist_matrix,
         clustering_method = "ward.D2", 
         #scale = "row", # scale = "row" - стандартизация по строкам
         cutree_rows = 3, # количество групп(кластеров)
         cutree_cols = length(colnames(final_data_scaled)),
         angle_col = 45)
```
Я разделила данные на 3 кластера. Видно, что в первом кластере находятся пациенты с низкими значением bwt, gest и lowph. В первом третьем кластерах находятся пациенты с низким apg1. В первом кластере наблюдаются пациенты с очень высоким hospstay, в то время как в третьем кластере наблюдаются случаи с самым мылым hospstay.

Показатели bwt, gest образуют один иерархический кластер, а apg1, lowph, pltct  - другой. hospstay группируется отдельно.


### Проведите PCA анализ на этих данных. Проинтерпретируйте результат. Нужно ли применять шкалирование для этих данных перед проведением PCA?

Если брать не шкалированные зараннее данные, то да. 

```{r}
library(FactoMineR)
library(ggbiplot) 

final_data.pca <- prcomp(final_data, 
                scale = T) 
summary(final_data.pca)

```
```{r}
 fviz_pca_var(final_data.pca, col.var = "contrib")
```

### Постройте biplot график для PCA. Раскрасьте его по значению колонки 'dead'.

```{r}
ggbiplot(final_data.pca, 
         scale=0, alpha = 0.5,
         groups = data_clean$dead,
         ellipse = T)
```

Переведите последний график в 'plotly'. При наведении на точку нужно, чтобы отображалось id пациента.

```{r}

data_clean$patient_id <- 1:nrow(data_clean)

library(plotly)


p <- ggbiplot(final_data.pca, 
              scale = 0,
              groups = data_clean$dead,
              ellipse = TRUE,
              alpha = 0.3) +
  geom_point(aes(text = paste("Patient ID:", data_clean$patient_id), 
                 color = data_clean$dead),  
             alpha = 0.3)


ggplotly(p, tooltip = "text")

```




```{r}
fviz_contrib(final_data.pca, choice = "var", axes = 1, top = 24) 
```


### Дайте содержательную интерпретацию PCA анализу. Почему использовать колонку 'dead' для выводов об ассоциации с выживаемостью некорректно? 


......



### Приведите ваши данные к размерности в две колонки через UMAP. Сравните результаты отображения точек между алгоритмами PCA и UMAP.


```{r}
library(tidymodels)
library(embed)
library(umap)


umap_prep <- recipe(~., data = final_data) %>%
  step_normalize(all_predictors()) %>%
  step_umap(all_predictors()) %>%
  prep() %>%
  juice() %>%
  mutate(dead = data_clean$dead, inout = data_clean$inout)  # Add `dead` and `inout` columns



umap_prep %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = dead, shape = inout), 
             alpha = 0.7, size = 2) +
  labs(color = NULL)

```

Для наших данных РСА не оптимальный метод, так как первые две компоненты объясняют всего 57%. В таких случаях можно попробовать UMAP, который позволяет лучше разделить данные на группы. 

Если в РСА наблюдения dead = 1 смещены вправо, то в UMAP они скорее смещены влево.



### Давайте самостоятельно увидим, что снижение размерности – это группа методов, славящаяся своей неустойчивостью. Измените основные параметры UMAP (n_neighbors и min_dist) и проанализируйте, как это влияет на результаты.


Все ровно как и в предыдущем коде, но не работает

```{r}
umap_prep2 <- recipe(~., data = final_data) %>%
  step_normalize(all_predictors()) %>%
  step_umap(all_predictors(), n_neighbors = 5, min_dist = 0.1) %>%
  prep() %>%
  juice() %>%
  mutate(dead = data_clean$dead, inout = data_clean$inout)


umap_prep2 %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = dead, shape = inout), 
             alpha = 0.7, size = 2) +
  labs(color = NULL)


```



### Давайте самостоятельно увидим, что снижение размерности – это группа методов, славящаяся своей неустойчивостью. Пермутируйте 50% и 100% колонки 'bwt'. Проведите PCA и UMAP анализ. Наблюдаете ли вы изменения в куммулятивном проценте объяснённой вариации PCA? В итоговом представлении данных на биплотах для PCA? Отличается ли визуализация данных?


```{r}
final_data_permuted <- final_data |>
  mutate(
    bwt_50 = ifelse(runif(n()) < 0.5, sample(bwt), bwt),
    bwt_100 = sample(bwt)
  )


final_data_permuted.pca <- prcomp(final_data_permuted, 
                scale = T) 
summary(final_data_permuted.pca)
```
Результат ухудшился, так как теперь первые две компоненты объясняют 47%.

Визуализация сильно не отличается.

```{r}
ggbiplot(final_data_permuted.pca, 
         scale=0, alpha = 0.5,
         groups = data_clean$dead,
         ellipse = T)
```
### Давайте проведем анализ чувствительности. Проведите анализ, как в шагах 4-6 для оригинального с удалением всех строк с пустыми значениями (т.е. включая колонки с количеством пропущенных значений больше 100), а затем для оригинального датафрейма с импутированием пустых значений средним или медианой. Как отличаются получившиеся результаты? В чем преимущества и недостатки каждого подхода?


```{r}
# 
data_no_na <- data |> 
  drop_na() |>
  mutate(across(
    where(~ is.numeric(.x) && max(.x, na.rm = TRUE) == 1 && min(.x, na.rm = TRUE) == 0),
         ~ as.factor(.)
  )) |>
  select(-birth, -year, -exit) |>
  select(where(~ is.numeric(.x) | is.integer(.x)))
  

#
data_clean_imp <- data |>
  mutate(across(
    where(~ is.numeric(.x) && max(.x, na.rm = TRUE) == 1 && min(.x, na.rm = TRUE) == 0),
    ~ as.factor(.)
  )) |>
  select(-birth, -year, -exit) |>
  select(where(~ is.numeric(.x) | is.integer(.x))) |>
  mutate(across(
    where(is.numeric),
    ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)
  )) 

```


```{r}

library(corrplot)

data_no_na %>% 
  cor() %>% 
  corrplot(
    order = 'hclust'
  )
```
В случае удаления строк с пропущенными значениями, появляется более заметная отрицательная корреляция между hospstay и bwt. 

```{r}
data_clean_imp %>% 
  cor() %>% 
  corrplot(
    order = 'hclust'
  )
```
В случае импутации пропущенных значений средним, результаты заметно отличаются. Практически ушли все отрицательные корреляции.

5-6 потом сделаю

### Давайте проведем анализ чувствительности. Сделайте то же, что в пункте 14, но для методов снижения размерности – PCA и UMAP. Проанализируйте результаты.



